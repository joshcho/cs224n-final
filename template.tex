\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{lipsum}

\newcommand{\note}[1]{\textcolor{blue}{{#1}}}

\title{
  Weighted Knowledge Graph Completion using Pre-trained Language Models \\
  \vspace{1em}
  \small{\normalfont Stanford CS224N Custom Project}  % Select one and delete the other
}

\author{
  Josh Cho \\
  Department of Computer Science \\
  Stanford University \\
  \texttt{joshcho@stanford.edu} \\
  % Examples of more authors
  % \And
  % Name \\
  % Department of Computer Science \\
  % Stanford University \\
  % \texttt{name@stanford.edu} \\
  % \And
  % Name \\
  % Department of Computer Science \\
  % Stanford University \\
  % \texttt{name@stanford.edu}
}

\begin{document}

\maketitle

\begin{abstract}
  Pre-trained Language Models (PLMs) demonstrate implicit understanding of domain knowledge. Knowledge graphs are explicit representations of domain knowledge. Since the rise of PLMs, there has been growing interest in rendering PLMs' implicit knowledge explicit via knowledge graphs. In domain knowledge, relations are often weighted, where there is strength in the relation. We build upon previous work to complete weighted knowledge graphs.
\end{abstract}

\section{Motivation for Preprocessing}

In the context of our work, preprocessing Wikipedia articles serves a specific purpose: determining the importance of triples in a knowledge graph. Knowledge graphs are powerful tools for representing complex relationships between entities, but they often lack information about the relative importance of each relationship. By preprocessing Wikipedia articles, we aim to derive importance scores for the edges in the graph, allowing us to better understand and prioritize the relationships between entities.

In many existing knowledge graphs, such as YAGO, edge weights are used to represent confidence scores. These scores indicate the level of certainty that a given relationship exists, as determined by experts. While confidence scores are valuable for assessing the reliability of relationships, they do not necessarily reflect the importance of those relationships in the context of the overall graph or the domain being represented.

Our preprocessing approach seeks to address this limitation by using Wikipedia articles as a rich source of information to assign importance scores to the triples in the knowledge graph. Since Wikipedia articles are curated by a large community of editors and follow a relatively consistent structure, they provide a solid foundation for estimating the importance of relationships between entities.

By preprocessing Wikipedia articles, we can extract meaningful information about entities, their attributes, and their relationships, and use this information to compute importance scores for the edges in the knowledge graph. These scores can help us understand the relative significance of each relationship, facilitating more effective analysis, decision-making, and prioritization of resources in downstream applications.

\section{Importance Scores}

In this section, we will discuss the process of preprocessing Wikipedia articles and the method used to compute importance scores for the generated triples. Preprocessing involves parsing the HTML content of the article and extracting the links (using the \textit{href} attribute).

For the importance scores, we use the position of the tail entity within the Wikipedia article as a proxy. Although this method has the disadvantage of not indicating the relation type, in most cases, there is one relation type between the head and tail entities. Therefore, the position of the tail entity is sufficient to determine the position of the "triple".

We employ a power decay function for computing the importance scores:

\begin{equation}
  I(x) = \alpha^x
\end{equation}

where $I(x)$ is the importance score, $x$ is the position of the tail entity in the article, and $\alpha$ is a positive constant. The power decay function is chosen because it assigns higher importance to entities mentioned earlier in the article while still considering entities that appear later. This function reflects the intuition that earlier-mentioned entities are generally more important, but it allows for the possibility that later-mentioned entities can still be relevant.

We consider the scenario of preprocessing a Wikipedia article on BERT and computing the positions and importance scores for the triples. This will illustrate the considerations to make when choosing an importance function. We will parse the HTML content and extract the links with appropriate Wikipedia \textit{href} attributes. Table \ref{table:1} shows the triples, their positions, and the importance scores calculated using the power decay function using $\alpha = 0.99$.

\begin{table}[h]
  \label{table:1}
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    Triple & Position & Importance Score \\
    \hline
    (BERT, language\_model, Language model) & 1 & $0.99^{1} \approx 0.99$ \\
    (BERT, publisher, Google) & 2 & $0.99^{2} \approx 0.9801$ \\
    (BERT, related\_work, BookCorpus) & 6 & $0.99^{6} \approx 0.9412$ \\
    (BERT, related\_work, English Wikipedia) & 7 & $0.99^{7} \approx 0.9321$ \\
    (BERT, based\_on, Transformer) & 9 & $0.99^{9} \approx 0.9135$ \\
    \hline
  \end{tabular}

  \caption{Positions and Importance Scores for Triples in the BERT Article}
\end{table}

Upon examining the example, we notice that the importance scores may not accurately reflect the significance of certain entities. For instance, the fact that BERT was trained on BookCorpus and English Wikipedia might be considered less important than the fact that BERT is based on the Transformer architecture. This observation suggests that the structure of Wikipedia articles may affect the computed importance scores, as entities mentioned towards the end of a section could be less important than entities mentioned at the beginning of the next section.

To address this issue, we could modify the importance scoring method to assign higher importance to entities appearing earlier within a section. One possible approach is to combine the power decay function with an additional weighting factor that takes into account the section's position in the article. This way, we can better capture the significance of entities and their relative importance within each section.

An alternative approach for determining importance scores would have been to search for raw text within the article. This method has its own set of advantages, such as being able to identify entities that might not be linked or entities that have different display text. However, this approach can be computationally more expensive, and it may not account for the variability in display text used for the same entity.

Another approach we could consider is to use the character position of the tail entity in the article rather than its position in the ordered list of links. This method would potentially provide a finer-grained measure of the entity's importance by taking into account its exact position in the text. However, it might be more susceptible to the influence of article structure and layout, which could introduce additional noise in the importance scores.


\section{Key Information to include}
\begin{itemize}
\item External collaborators: None
\item External mentor: None
\item Sharing project: N/A
\end{itemize}

\section{Background}
A knowledge graph $\mathcal{KG}$ can be defined as:
\begin{equation}
  \mathcal{KG} = \{\mathcal{E}, \mathcal{R}, \mathcal{T}\}
\end{equation}
where $\mathcal{E}$ is the set of entities and $\mathcal{R}$ is the set of relations. $\mathcal{T}$ is the set of triples in the form of $(h,r,t)$ where head and tail $h,t \in \mathcal{E}$ and relation $r \in \mathcal{R}$.

The task of Knowledge Graph Completion (KGC) is as follows: given relation $r$ and entities $h, t$, we label if $(h,r,t)$ is true.

A weighted knowledge graph augments $\mathcal{R}$ such that $(h,r,t,w) \in \mathcal{R}$, where $w \in [0,1]$. $w$ is the weight of the relation. Thus given $h,r,t$ as before, weighted KGC estimates $w_{h,r,t}$.

\section{Approach}
We build upon KG-BERT\cite{https://doi.org/10.48550/arxiv.1909.03193} to support weighted knowledge graphs. We generate a weighted knowledge graph using DBpedia's NLP Interchange Format (NIF)\cite{DBLP:journals/corr/abs-1812-10315} which provides the full text along with the relations. Thus given head $h$, relation $r$, and tail $t$, let $f_{t,h}$ be the frequency of $t$ in the full text of $h$. Then we model the weight with the softmax function:
\begin{equation}
  w_{h,r,t} = \frac{\exp(f_{t,h})}{\sum_{t'} \exp(f_{t',h})}
\end{equation}

\section{Experiments}
This section is expected to contain the following.
\begin{itemize}
\item \textbf{Data}: We are using WN11\cite{10.5555/2999611.2999715} (a subset of WordNet), FB13\cite{10.5555/2999792.2999923} (a subset of Freebase), and DBpedia. WN11 and FB13 are used for experimenting with KG-BERT, and DBpedia will be used to construct the data for weighted KGC.
\item \textbf{Evaluation method}: The original KG-BERT uses Cross-Entropy Loss. In order to support continuous values, we use Mean-squared Error:
  \begin{equation}
    \mathcal{L} = \frac{1}{n} \sum_{i = 1}^n (y_i - \tilde{y}_i)^2
  \end{equation}
  where $y_i$ is the actual label and $\tilde{y}_i$ is the predicted label (i.e. weights of relation).
\item \textbf{Experimental details}: As with KG-BERT, we chose pre-trained BERT-Base model with 12 layers, 12 self-attention heads, and $H = 768$ as initialization. This model is then fine-tuned with Adam implemented in BERT, where our approach differs from KG-BERT. We maintain the hyper-parameters in KG-BERT fine-tuning: batch size of 32, learning rate of 5e-5, and dropout rate of 0.1.

\item \textbf{Results}: We are currently constructing the data from DBpedia, so in the meantime we converted the KG-BERT model to use regression. The following table is for WN11.

  \begin{center}
    \begin{tabular}{ |c|c|c|c| }
      \hline
      & KG-BERT & KG-BERT Modified & KG-BERT with regression \\
      Evaluation Loss & 0.278 & 0.273 & 0.0585 \\
      Accuracy & 93.1\% & 93.3\% & N/A  \\
      \hline
    \end{tabular}
  \end{center}

  where KG-BERT Modified adds a minor optimization where relations are converted to more humanlike language.
\end{itemize}


\section{Future work}
We will test the regression model with data constructed from DBpedia using our weights construction. In addition, we will explore the techniques in PKGC paper\cite{lv-etal-2022-pre}.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}